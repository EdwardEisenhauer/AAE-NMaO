\subsection{Problem 1}%
\label{sec:problem_1}
Solve the following system with the selected iterative solvers
\begin{equation*}
  \systeme{2u-v=0,-u+2v-w=0,-v+2w-z=0,-w+2z=5}
\end{equation*}
Estimate the computational costs and convergence rates.
Start the iterations from zero-value initial guess.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{Mathematics}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The principle of the iterative linear solvers of the linear system of equations
expressed as $\matr{A}\matr{x}=\matr{b}$ is to start with some initial guess
$\matr{x}^{(0)}$ and approximate the solution with each iteration.

Stationary basic iterative methods, such as those presented in~\nameref{sec:algorithms},
perform a linear transformation $\matr{G}$ of the initial guess $\matr{x}^{(0)}$ at each
iteration:
\begin{equation*}
  \matr{x}^{(k+1)} = \matr{G}\matr{x}^{(k)} + \matr{c}
\end{equation*}

For any starting guess $\matr{x}^{(0)}$ the iterations are convergent to the real
solution $\matr{x}^{*}$ if and only if all of the eigenvalues of $\matr{G}$ are smaller
than $1$.

Assuming we know the exact solution $\matr{x}^*$, we may define the
\textit{solution error} $e$:
\begin{equation*}
  e^{(k)} = {\left\lVert\matr{x}^{*}-\matr{x}^{(k)}\right\rVert}_2
\end{equation*}
The method converges if the solution error converges to zero:
\begin{equation*}
  \lim_{k\to\infty}{e^{(k)}} = 0
  \quad \text{if} \quad
  \rho\left(\matr{G}^{(k)}\right) < 1
\end{equation*}
where $\rho$ is the \textit{spectral radius}:
\begin{equation*}
  \rho(\matr{G}) = \max_i{\lvert\lambda_i\rvert} = \lVert\matr{\Lambda}\rVert_2
\end{equation*}

Following~\cite{Zdunek}, the value of the solution error at the $k$-th iteration may be
calculated via:
\begin{equation}
  \matr{e}^{(k)} = \matr{G}^k * \matr{e}^{(0)}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Spectral radius
One of steps that can allow us to check if iterations $\{x_k\}$ are convergent to $\matr{x} = \matr{A}^-1\matr{b}$ would be checking spectral radius.

\begin{equation*}
  \rho(\matr{S}^-1\matr{T}) = \max \lvert\lambda_i\rvert
\end{equation*}
\begin{itemize}
  \item Gauss-Seidel\\
    
    $ \matr{A} = \matr{S} - \matr{T} $
    where $ \matr{S} $ is the lower triangular part of $ \matr{A} $ including the diagonal, and $ \matr{T} $ is the strict upper triangular part of $ \matr{A} $.

    The iterative formula for the Gauss-Seidel method is
    $ \matr{Sx}_{k+1} = \matr{Tx}_k + \matr{b}, $
    which can be rewritten as
    $ \matr{x}_{k+1} = \matr{Gx}_k + \matr{c}, $
    where $ \matr{G} = \matr{S}^{-1}\matr{T} $ and $ \matr{c} = \matr{S}^{-1}\matr{b} $.

    The convergence of the Gauss-Seidel method is determined by the spectral radius of $ \matr{G} $, denoted by $ \rho(\matr{G}) $. The method converges if $ \rho(\matr{G}) < 1 $.

    To find $ \rho(\matr{G}) $, we first decompose $ \matr{A} $ into $ \matr{D} $, $ \matr{L} $, and $ \matr{U} $, where $ \matr{D} $ is the diagonal of $ \matr{A} $, $ \matr{L} $ is the strict lower triangular part of $ \matr{A} $, and $ \matr{U} $ is the strict upper triangular part of $ \matr{A} $. Then, we compute $ \matr{G} $ as follows:
    $ \matr{G} = (\matr{D} + \matr{L})^{-1}\matr{U}. $

    The spectral radius $ \rho(\matr{G}) $ is the maximum absolute value of the eigenvalues of $ \matr{G} $, given by
    $ \rho(\matr{G}) = \max_i |\lambda_i|, $
    where $ \lambda_i $ are the eigenvalues of $ \matr{G} $.

    Given a matrix $ \matr{A} $, the spectral radius $ \rho(\matr{G}) $ for the Gauss-Seidel method was calculated to be approximately 0.6, indicating that the iterations will converge.
  
  \item Jacobi \\
    For the Jacobi method, the splitting of matrix $ \matr{A} $ is slightly different:
    $ \matr{A} = \matr{D} - \matr{R}, $
    where $ \matr{D} $ is the diagonal part of $ \matr{A} $, and $ \matr{R} $ is the remainder of $ \matr{A} $ (i.e., $ \matr{R} = \matr{L} + \matr{U} $).

    The iteration formula for the Jacobi method is:
    $ \matr{Dx}_{k+1} = \matr{Rx}_k + \matr{b}, $
    which can be rearranged to get the new estimate of $ \matr{x} $ at each iteration:
    $ \matr{x}_{k+1} = \matr{D}^{-1}(\matr{b} - \matr{Rx}_k). $

    In this case, the iteration matrix $ \matr{G} $ for the Jacobi method is defined as:
    $ \matr{G} = \matr{D}^{-1}\matr{R}. $

    The spectral radius of the iteration matrix, $ \rho(\matr{G}) = 0.8090$

  \item Landwebber
  The convergence of the Richardson's first-order method, depends on the appropriate choice of the relaxation parameter \( \alpha \). 
  For a given matrix $\matr{A}$, the iterations converge to the least-squares solution \( x = A^+b \) if \( \alpha \) is chosen within the range:
  \begin{equation*}
    0 < \alpha < \frac{2}{\lambda_{max}}(\matr{A}^T\matr{A}) 
  \end{equation*}

  where  $\lambda_{max}(\matr{A}^T\matr{A})$ is the largest eigenvalue of $ \matr{A}^T\matr{A} $.

  Given the matrix $\matr{A}$ used in previous sections, we compute $\max{A}^T\matr{A}$ and then find its eigenvalues. 
  The maximum eigenvalue from this set determines the upper limit for the relaxation parameter. 
  
  The calculation is as follows:
  \begin{enumerate}
      \item Compute $ \matr{A}^T\max{A} $:
      \begin{equation*}
          A^TA = \matr{A}^\top \matr{A}
      \end{equation*}
      
      \item Find the eigenvalues of $\matr{A}^T\matr{A}$, denoted $\lambda_i$.
      
      \item Identify the maximum eigenvalue:
      \begin{equation*}
        \lambda_{max} = \max(\lambda_i)
      \end{equation*}
      
      \item Calculate the upper bound for \( \alpha \):
      \begin{equation*}
        \alpha < \frac{2}{\lambda_{max}}
      \end{equation*}
  \end{enumerate}

  Computed result using MATLAB gives us $\lambda_{max} \approx 13.0902$ giving us resulting convergence criterion range as:
  \begin{equation*}
    0 < \alpha < 0.1528
  \end{equation*}

  Choosing \( \alpha \) within this range ensures that the Landweber iterations will converge.

\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{Solving}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{Solution}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Since the considered system of linear equations is simple and consistent, we may use the
\MATLAB's backslash operator in the form of \lstinline[style=Matlab-editor]{A\b}, to
determine the exact solution:
\begin{equation*}
  \matr{A} = \begin{bmatrix}
    \phantom{-}2 & -1 & \phantom{-}0 & \phantom{-}0 \\
    -1 & \phantom{-}2 & -1 & \phantom{-}0 \\
    \phantom{-}0 & -1 & \phantom{-}2 & -1 \\
    \phantom{-}0 & \phantom{-}0 & -1 & \phantom{-}2
  \end{bmatrix}, \qquad
  \matr{b} = \begin{bmatrix}
    0 \\
    0 \\
    0 \\
    5
  \end{bmatrix}, \qquad
  \matr{x}^* = \begin{bmatrix}
    1 \\
    2 \\
    3 \\
    4
  \end{bmatrix}
\end{equation*}
Before proceeding with approximating the solution, we check the residual error
at the $k$-th iteration for each of the implemented algorithms to verify if the method
converges. The convergence criterion is:

Let's verify it for each of our algorithms:
\lstinputlisting[style=Matlab-editor]{problems/Problem_1.m}

\todo[inline]{Calculate the solution error for each method.}
\todo[inline]{If the error converges --- apply the method.}
